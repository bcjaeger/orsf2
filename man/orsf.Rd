% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/orsf.R
\name{orsf}
\alias{orsf}
\alias{orsf_train}
\title{Oblique Random Survival Forest (ORSF)}
\usage{
orsf(
  data,
  formula,
  control = orsf_control_cph(),
  weights = NULL,
  n_tree = 500,
  n_split = 5,
  n_retry = 3,
  mtry = NULL,
  leaf_min_events = 1,
  leaf_min_obs = 5,
  split_min_events = 5,
  split_min_obs = 10,
  split_min_stat = 3.841459,
  oobag_pred = TRUE,
  oobag_pred_horizon = NULL,
  oobag_eval_every = n_tree,
  oobag_fun = NULL,
  importance = "anova",
  tree_seeds = NULL,
  attach_data = TRUE,
  no_fit = FALSE,
  ...
)

orsf_train(object)
}
\arguments{
\item{data}{(\emph{data.frame}) that contains the relevant variables.}

\item{formula}{(\emph{formula}) The response on the left hand side should
include a time variable, followed by a status variable (see examples).
The terms on the right are names of predictor variables. A \code{.}
symbol on the right hand side is short-hand for using all variables
in \code{data} as predictors.}

\item{control}{An \code{aorsf_control} object, created with \link{orsf_control_net}
or \link{orsf_control_cph}. Default is \code{control = orsf_control_cph()}.}

\item{weights}{(\emph{numeric vector}) Optional. If given, this
input should be a vector with length equal to the number of rows in
\code{data}. The values in \code{weights} are treated like replication weights,
i.e., a weight value of 2 is the same thing as having 2 observations
in \code{data}, each containing a copy of the corresponding person's data.
\strong{Use this input cautiously}, as \code{orsf} will count the number of
observations and events prior to growing a node for a tree, so higher
values in \code{weights} will lead to deeper trees.}

\item{n_tree}{(\emph{integer}) the number of trees to grow.
Default is \code{n_tree = 500.}}

\item{n_split}{(\emph{integer}) the number of cut-points assessed when splitting
a node in decision trees. Default is \code{n_split = 5}.}

\item{n_retry}{(\emph{integer}) when a node can be split, but the current
linear combination of inputs is unable to provide a valid split, \code{orsf}
will try again with a new linear combination based on a different set
of randomly selected predictors, up to \code{n_retry} pred_horizon. When
\code{n_retry = 0} the retry mechanic is not applied.
Default is \code{n_retry = 3}.}

\item{mtry}{(\emph{integer}) Number of predictors randomly selected as candidates
for splitting a node. The default is the smallest integer greater than
the square root of the number of total predictors, i.e.,
\verb{mtry = ceiling(sqrt(number of predictors))}}

\item{leaf_min_events}{(\emph{integer}) minimum number of events in a
leaf node. Default is \code{leaf_min_events = 1}}

\item{leaf_min_obs}{(\emph{integer}) minimum number of observations in a
leaf node. Default is \code{leaf_min_obs = 5}}

\item{split_min_events}{(\emph{integer}) minimum number of events required
in a node to consider splitting it. Default is \code{split_min_events = 5}}

\item{split_min_obs}{(\emph{integer}) minimum number of observations required
in a node to consider splitting it. Default is \code{split_min_obs = 10}.}

\item{split_min_stat}{(double) minimum test statistic required to split
a node. Default is 3.841459 for the log-rank test, which is roughly
a p-value of 0.05}

\item{oobag_pred}{(\emph{logical}) if \code{TRUE} out-of-bag predictions are returned
in the \code{aorsf} object. Default is \code{TRUE}.}

\item{oobag_pred_horizon}{(\emph{numeric}) A numeric value indicating what time
should be used for out-of-bag predictions. Default is the median
of the observed times, i.e., \code{oobag_pred_horizon = median(time)}.}

\item{oobag_eval_every}{(\emph{integer}) The out-of-bag performance of the
ensemble will be checked every \code{oobag_eval_every} trees. So, if
\code{oobag_eval_every = 10}, then out-of-bag performance is checked
after growing the 10th tree, the 20th tree, and so on. Default
is \code{oobag_eval_every = n_tree}, so that out-of-bag performance is
assessed once after growing all the trees.}

\item{oobag_fun}{(\emph{function}) A function to be measure the accuracy of
out-of-bag predictions. When \code{oobag_fun} = \code{NULL} (the default),
out-of-bag predictions are evaluated using Harrell's C-statistic.
For more details see the
\href{https://bcjaeger.github.io/aorsf/articles/oobag.html#user-supplied-out-of-bag-evaluation-functions}{vignette}
on out-of-bag predictions:}

\item{importance}{(\emph{character}) Indicate method for variable importance:
\itemize{
\item 'none': no variable importance is computed.
\item 'anova': use the analysis of variance (ANOVA) method
\item 'negate': compute negation importance
\item 'permute': compute permutation importance
}}

\item{tree_seeds}{(\emph{integer vector}) if specified, random seeds will be set
using the values in \code{tree_seeds[i]}  before growing tree i. Two forests
grown with the same number of trees and the same seeds will have the exact
same out-of-bag samples and, in many cases, the same random sets of
candidate predictors. This design makes comparisons of out-of-bag error
between two random forests more meaningful, since the out-of-bag
performance of a random forest depends somewhat on the observations
picked in out-of-bag samples. If \code{tree_seeds} is \code{NULL} (the default),
no seeds are set during the training process.}

\item{attach_data}{(\emph{logical}) if \code{TRUE}, a copy of the training
data will be attached to the output. This is helpful if you
plan on using functions like \link{orsf_pd_summary} to interpret the fitted
forest using its training data. Default is \code{TRUE}.}

\item{no_fit}{(\emph{logical}) if \code{TRUE}, pre-processing steps are defined and
parametrized, but training is not initiated. The object returned can be
directly submitted to \code{orsf_train()} so long as \code{attach_data} is \code{TRUE}.}

\item{...}{Further arguments passed to or from other methods
(not currently used).}

\item{object}{an untrained 'aorsf' object, created by setting
\code{no_fit = TRUE} in \code{orsf()}.}
}
\value{
an accelerated oblique RSF object (\code{aorsf})
}
\description{
The oblique random survival forest (ORSF) is an extension of the RSF
algorithm developed by Ishwaran et al and maintained in the
\code{RandomForestSRC} package. The difference between ORSF and RSF is
that ORSF uses linear combinations of input variables whereas RSF
uses a single variable when growing new nodes in survival decision trees.
A linear combination is an expression constructed from a set of terms
by multiplying each term by a constant and adding the results (e.g.
a linear combination of x and y would be any expression of the form
ax + by, where a and b are constants). For more details on the ORSF
algorithm, see Jaeger et al, 2019. The \code{orsf()} function implements a
novel algorithm that speeds up the ORSF algorithm described by Jaeger
et al (see details).
}
\details{
This function is based on and highly similar to the \code{ORSF} function
in the \code{obliqueRSF} R package. The primary difference is that this
function runs much faster. The speed increase is attributable to better
management of memory (i.e., no unnecessary copies of inputs) and using
a Newton Raphson scoring algorithm to identify linear combinations of
inputs rather than performing penalized regression using routines in
\code{glmnet}.The modified Newton Raphson scoring algorithm that this
function applies is an adaptation of the C++ routine developed by
Terry M. Therneau that fits Cox proportional hazards models
(see \code{\link[survival:coxph]{survival::coxph()}} and more specifically \code{\link[survival:agreg.fit]{survival::coxph.fit()}}).

\strong{Some comments on inputs}

\emph{formula}: The response in \code{formula} can be a survival
object as returned by the \link[survival:Surv]{survival::Surv} function,
but can also just be the time and status variables.
For example, \code{Surv(time, status) ~ .} works just like
\code{time + status ~ .}. The only thing that can break this
input is putting the variables in the wrong order, i.e.,
writing \code{status + time ~ .} will make \code{orsf} assume your
\code{status} variable is actually the \code{time} variable.

\emph{mtry}: The \code{mtry} parameter may be temporarily reduced to ensure there
are at least 2 events per predictor variable. This occurs when using
\link{orsf_control_cph} because coefficients in the Newton Raphson scoring
algorithm may become unstable when the number of covariates is
greater than or equal to the number of events. This reduction does not
occur when using \link{orsf_control_net}.

\emph{oobag_fun}: The function must have two inputs: \code{y_mat} and \code{s_vec}.
\itemize{
\item The input \code{y_mat} is presumed to be a matrix with two columns
named \code{time} (first column) and \code{status} (second column).
\item The input \code{s_vec} is presumed to be a numeric vector containing
predicted survival probabilities for \code{y_mat}.
}

If \code{oobag_fun} is specified, it will be used in the
computation of negation importance or permutation importance, but it
will not have any role for ANOVA importance.

\emph{importance}: See \link{orsf_vi} for descriptions of the available methods.

\strong{What is an oblique decision tree?}

Decision trees are developed by splitting a set of training data into two
new subsets, with the goal of having more similarity within the new subsets
than between them. This splitting process is repeated on the resulting
subsets of data until a stopping criterion is met. When the new subsets of
data are formed based on a single predictor, the decision tree is said to
be axis-based because the splits of the data appear perpendicular to the
axis of the predictor. When linear combinations of variables are used
instead of a single variable, the tree is oblique because the splits of
the data are neither parallel nor at a right angle to the axis

\emph{Figure} : Decision trees for classification with axis-based splitting
(left) and oblique splitting (right). Cases are orange squares; controls
are purple circles. Both trees partition the predictor space defined by
variables X1 and X2, but the oblique splits do a better job of separating
the two classes.

\if{html}{\figure{tree_axis_v_oblique.png}{options: width=95\%}}
}
\examples{

# standard workflow for model development: fit and interpret

fit <- orsf(pbc_orsf, formula = Surv(time, status) ~ . - id)

print(fit)

# more specific parameters

fit_custom <- orsf(pbc_orsf,
                   formula = time + status ~ . - id,
                   mtry = 2,
                   oobag_pred_horizon = 4000,
                   oobag_eval_every = 50)

# 10 oobag error values are computed  b/c oob error
# is assessed every 50 trees (50, 100, ..., 500)
fit_custom$eval_oobag$stat_values

# make your own oobag function:
# (this is an R implementation of Harrell's C
# stat, which is what orsf uses by default)

oobag_c_harrell <- function(y_mat, s_vec){
 time = y_mat[, 1]
 status = y_mat[, 2]
 events = which(status == 1)
 k = nrow(y_mat)
 total <- 0
 concordant <- 0
 for(i in events){
  if(i+1 <= k){
   for(j in seq(i+1, k)){
    if(time[j] > time[i]){
     total <- total + 1
     if(s_vec[j] > s_vec[i]){
      concordant <- concordant + 1
     } else if (s_vec[j] == s_vec[i]){
      concordant <- concordant + 0.5
     }
    }
   }
  }
 }
 concordant / total
}

# tree_seeds lets you grow each tree with the seed it corresponds to
# (nice way to make sure my R oobag function gives the same answer
# as the internal C oobag function)

fit_custom_oobag <- orsf(pbc_orsf,
                         formula = Surv(time, status) ~ . - id,
                         oobag_fun = oobag_c_harrell,
                         n_tree = 10,
                         tree_seeds = 1:10)

fit_standard_oobag <- orsf(pbc_orsf,
                           formula = Surv(time, status) ~ . - id,
                           n_tree = 10,
                           tree_seeds = 1:10)

fit_standard_oobag$eval_oobag$stat_values

fit_custom_oobag$eval_oobag$stat_values


\dontrun{requires too many external packages

# --------------------------------------------------------------------------
# a standard internal validation workflow using aorsf and tidymodels
# --------------------------------------------------------------------------


library(tidymodels)
library(tidyverse)
library(survivalROC)
library(aorsf)

set.seed(329)

# a recipe to impute missing values instead of discarding them.
# (this is for illustration only. pbc_orsf does not have missing values)

imputer <- recipe(x = pbc_orsf, time + status ~ .) |>
 step_impute_mean(all_numeric_predictors()) |>
 step_impute_mode(all_nominal_predictors()) |>
 step_rm(id)


# 10-fold cross validation; make a container for the pre-processed data
analyses <- vfold_cv(data = pbc_orsf, v = 10) |>
 mutate(recipe = map(splits, ~prep(imputer, training = training(.x))),
        data = map(recipe, juice),
        data_test = map2(splits, recipe, ~bake(.y, new_data = testing(.x))))

# 10-fold cross validation; train models and compute test predictions
aorsf_data <- analyses |>
 select(data, data_test) |>
 mutate(fit = map(data, orsf, formula = time + status ~ .),
        pred = map2(fit, data_test, predict, pred_horizon = 3500),
        pred = map(pred, as.numeric))

# testing sets are small, so pool them and compute 1 overall C-stat.
aorsf_eval <- aorsf_data |>
 select(data_test, pred) |>
 unnest(cols = everything()) |>
 summarize(
  auc = survivalROC(Stime = time,
                    status = status,
                    marker = pred,
                    predict.time = 3500,
                    span = 0.25*n()^(-0.20)) |>
   getElement('AUC')
 )

# C-stat: 0.81465
aorsf_eval$auc

}
}
\references{
Breiman L. Random forests. \emph{Machine learning}. 2001 Oct;45(1):5-32.
DOI: 10.1023/A:1010933404324

Ishwaran H, Kogalur UB, Blackstone EH, Lauer MS. Random survival forests.
\emph{Annals of applied statistics}. 2008 Sep;2(3):841-60.
DOI: 10.1214/08-AOAS169

Jaeger BC, Long DL, Long DM, Sims M, Szychowski JM, Min YI,
Mcclure LA, Howard G, Simon N. Oblique random survival forests.
\emph{Annals of applied statistics}. 2019 Sep;13(3):1847-83.
DOI: 10.1214/19-AOAS1261

Jaeger BC, Welden S, Lenoir K, Speiser JL, Segar M, Pandey A, Pajewski NM.
Accelerated and interpretable oblique random survival forests.
arXiv e-prints. 2022 Aug 3:arXiv-2208.
URL: https://arxiv.org/abs/2208.01129

Harrell FE, Califf RM, Pryor DB, Lee KL, Rosati RA.
Evaluating the Yield of Medical Tests. \emph{JAMA}. 1982;247(18):2543–2546.
DOI: 10.1001/jama.1982.03320430047030
}


# Examples

```{r, echo=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

First we load some relevant packages

```{r}

set.seed(329730)
suppressPackageStartupMessages({
 library(aorsf)
 library(survival)
 library(tidymodels)
 library(tidyverse)
 library(randomForestSRC)
 library(ranger)
 library(riskRegression) 
 library(obliqueRSF)
})

```


The entry-point into `aorsf` is the standard call to `orsf()`:

```{r}

fit <- orsf(pbc_orsf, Surv(time, status) ~ . - id)

```

printing `fit` provides quick descriptive summaries:

```{r}
fit
```

## Model control

For these examples we will make use of the `orsf_control_` functions to build and compare models based on their out-of-bag predictions. We will also standardize the out-of-bag samples using the input argument `tree_seeds`

### Accelerated linear combinations

The accelerated ORSF ensemble is the default because it has a nice balance of computational speed and prediction accuracy. It runs a single iteration of Newton Raphson scoring on the Cox partial likelihood function to find linear combinations of predictors.


```{r}

fit_accel <- orsf(pbc_orsf, 
                  control = orsf_control_fast(),
                  formula = Surv(time, status) ~ . - id,
                  tree_seeds = 329)

```

### Linear combinations with Cox regression

`orsf_control_cph` runs Cox regression in each non-terminal node of each survival tree, using the regression coefficients to create linear combinations of predictors:

```{r}

fit_cph <- orsf(pbc_orsf, 
                control = orsf_control_cph(),
                formula = Surv(time, status) ~ . - id,
                tree_seeds = 329)

```

### Linear combinations with penalized cox regression

`orsf_control_net` runs penalized Cox regression in each non-terminal node of each survival tree, using the regression coefficients to create linear combinations of predictors. This can be really helpful if you want to do feature selection within the node, but it is a lot slower than the other options.

```{r}

# select 3 predictors out of 5 to be used in
# each linear combination of predictors.
fit_net <- orsf(pbc_orsf, 
                control = orsf_control_net(df_target = 3),
                formula = Surv(time, status) ~ . - id,
                tree_seeds = 329)

```

### Linear combinations with your own function 

Let's make three customized functions to identify linear combinations of predictors. 

- The first uses random coefficients

  ```{r}
  
  f_rando <- function(x_node, y_node, w_node){
   matrix(runif(ncol(x_node)), ncol=1) 
  }
  
  ```

- The second derives coefficients from principal component analysis.

  ```{r}
  
  f_pca <- function(x_node, y_node, w_node) { 
   
   # estimate two principal components.
   pca <- stats::prcomp(x_node, rank. = 2)
   # use the second principal component to split the node
   pca$rotation[, 1L, drop = FALSE]
   
  }
  
  ```

- The third uses `orsf()` inside of `orsf()`.

  ```{r}
  # This approach is known as reinforcement learning trees.  
  # some special care is taken to prevent your R session from crashing.
  # Specifically, random coefficients are used when n_obs <= 10
  # or n_events <= 5. 
  
  f_aorsf <- function(x_node, y_node, w_node){
   
   colnames(y_node) <- c('time', 'status')
   colnames(x_node) <- paste("x", seq(ncol(x_node)), sep = '')
   
   data <- as.data.frame(cbind(y_node, x_node))
  
   if(nrow(data) <= 10 || sum(y_node[,'status']) <= 5) 
    return(matrix(runif(ncol(x_node)), ncol = 1))
    
   fit <- orsf(data, time + status ~ ., 
               weights = as.numeric(w_node),
               n_tree = 25,
               importance = 'permute')
   
   out <- orsf_vi(fit)
   
   # drop the least two important variables
   n_vars <- length(out)
   out[c(n_vars, n_vars-1)] <- 0
   
   # ensure out has same variable order as input
   out <- out[colnames(x_node)]
   
   matrix(out, ncol = 1)
   
  }
  
  ```

We can plug these functions into `orsf_control_custom()`, and then pass the result into `orsf()`:

```{r}

fit_rando <- orsf(pbc_orsf,
                  Surv(time, status) ~ . - id,
                  control = orsf_control_custom(beta_fun = f_rando),
                  tree_seeds = 329)

fit_pca <- orsf(pbc_orsf,
                Surv(time, status) ~ . - id,
                control = orsf_control_custom(beta_fun = f_pca),
                tree_seeds = 329)

fit_rlt <- orsf(pbc_orsf, time + status ~ . - id, 
                control = orsf_control_custom(beta_fun = f_aorsf),
                tree_seeds = 329)

```

So which fit seems to work best in this example? Let's find out by evaluating the out-of-bag survival predictions.

```{r}

risk_preds <- list(
 accel = 1 - fit_accel$pred_oobag,
 cph   = 1 - fit_cph$pred_oobag,
 net   = 1 - fit_net$pred_oobag,
 rando = 1 - fit_rando$pred_oobag,
 pca   = 1 - fit_pca$pred_oobag,
 rlt   = 1 - fit_rlt$pred_oobag
)

sc <- Score(object = risk_preds, 
            formula = Surv(time, status) ~ 1, 
            data = pbc_orsf, 
            summary = 'IPA',
            times = fit_accel$pred_horizon)

```

The AUC values, from highest to lowest:

```{r}
sc$AUC$score[order(-AUC)]
```

And the indices of prediction accuracy:

```{r}
sc$Brier$score[order(-IPA), .(model, times, IPA)]
```

From inspection,

- `net`, `accel`, and `rlt` have high discrimination and index of prediction accuracy.

- `rando` and `pca` do less well, but they aren't bad.

## tidymodels

This example uses `tidymodels` functions but stops short of using an official `tidymodels` workflow. I am working on getting `aorsf` pulled into the `censored` package and I will update this with real workflows if that happens!

### Comparing ORSF with other learners

Start with a recipe to pre-process data

```{r}

imputer <- recipe(pbc_orsf, formula = time + status ~ .) %>% 
 step_impute_mean(all_numeric_predictors()) %>%
 step_impute_mode(all_nominal_predictors()) 

```

Next create a 10-fold cross validation object and pre-process the data:

```{r}

# 10-fold cross validation; make a container for the pre-processed data
analyses <- vfold_cv(data = pbc_orsf, v = 10) %>%
 mutate(recipe = map(splits, ~prep(imputer, training = training(.x))),
        train = map(recipe, juice),
        test = map2(splits, recipe, ~bake(.y, new_data = testing(.x))))

analyses

```

Define functions for a 'workflow' with `randomForestSRC`, `ranger`, and `aorsf`. 

```{r}

rfsrc_wf <- function(train, test, pred_horizon){
 
 # rfsrc does not like tibbles, so cast input data into data.frames
 train <- as.data.frame(train)
 test <- as.data.frame(test)
 
 rfsrc(formula = Surv(time, status) ~ ., data = train) %>% 
  predictRisk(newdata = test, times = pred_horizon) %>% 
  as.numeric()
 
}

ranger_wf <- function(train, test, pred_horizon){
 
 ranger(Surv(time, status) ~ ., data = train) %>% 
  predictRisk(newdata = test, times = pred_horizon) %>% 
  as.numeric()
 
}

aorsf_wf <- function(train, test, pred_horizon){
 
 train %>% 
  orsf(Surv(time, status) ~ .,) %>% 
  predict(new_data = test, pred_horizon = pred_horizon) %>% 
  as.numeric()
 
}

```

Run the 'workflows' on each fold:

```{r}

# 5 year risk prediction
ph <- 365.25 * 5

results <- analyses %>% 
 transmute(test, 
           pred_aorsf = map2(train, test, aorsf_wf, pred_horizon = ph),
           pred_rfsrc = map2(train, test, rfsrc_wf, pred_horizon = ph),
           pred_ranger = map2(train, test, ranger_wf, pred_horizon = ph))

```

Next unnest each column to get back a `tibble` with all of the testing data and predictions.

```{r}

results <- results %>% 
 unnest(everything())

glimpse(results)

```

And finish by aggregating the predictions and computing performance in the testing data. Note that I am computing one statistic for all predictions instead of computing one statistic for each fold. This approach is fine when you have smaller testing sets and/or small event counts.

```{r}

Score(
 object = list(aorsf = results$pred_aorsf,
               rfsrc = results$pred_rfsrc,
               ranger = results$pred_ranger),
 formula = Surv(time, status) ~ 1, 
 data = results, 
 summary = 'IPA',
 times = ph
)

```

From inspection, 

- `aorsf` obtained slightly higher discrimination (AUC)

- `aorsf` obtained higher index of prediction accuracy (IPA)


## mlr3 pipelines

__Warning__: this code may or may not run depending on your current version of `mlr3proba`. First we load some additional `mlr3` libraries.

```{r}

suppressPackageStartupMessages({
 library(mlr3verse)
 library(mlr3proba)
 library(mlr3extralearners)
 library(mlr3viz)
 library(mlr3benchmark)
})

```

Next we'll define some tasks for our learners to engage with.

```{r, eval = FALSE}

# Mayo Clinic Primary Biliary Cholangitis Data
task_pbc <- 
 TaskSurv$new(
  id = 'pbc',  
  backend = select(pbc_orsf, -id) %>% 
   mutate(stage = as.numeric(stage)),  
  time = "time", 
  event = "status"
 )

# Veteran's Administration Lung Cancer Trial
data(veteran, package = "randomForestSRC")

task_veteran <- 
 TaskSurv$new(
  id = 'veteran',  
  backend = veteran,  
  time = "time", 
  event = "status"
 )

# NKI 70 gene signature
data_nki <- OpenML::getOMLDataSet(data.id = 1228)

task_nki <- 
 TaskSurv$new(
  id = 'nki',  
  backend = data_nki$data,  
  time = "time", 
  event = "event"
 )

# Gene Expression-Based Survival Prediction in Lung Adenocarcinoma
data_lung <- OpenML::getOMLDataSet(data.id = 1245)

task_lung <- 
 TaskSurv$new(
  id = 'nki',  
  backend = data_lung$data %>% 
   mutate(OS_event = as.numeric(OS_event) -1),  
  time = "OS_years", 
  event = "OS_event"
 )


# Chemotherapy for Stage B/C colon cancer
# (there are two rows per person, one for death 
#  and the other for recurrence, hence the two tasks)

task_colon_death <-
 TaskSurv$new(
  id = 'colon_death',  
  backend = survival::colon %>%
   filter(etype == 2) %>% 
   drop_na() %>% 
   # drop id, redundant variables
   select(-id, -study, -node4, -etype),
   mutate(OS_event = as.numeric(OS_event) -1),  
  time = "time", 
  event = "status"
 )

task_colon_recur <-
 TaskSurv$new(
  id = 'colon_death',  
  backend = survival::colon %>%
   filter(etype == 1) %>% 
   drop_na() %>% 
   # drop id, redundant variables
   select(-id, -study, -node4, -etype),
   mutate(OS_event = as.numeric(OS_event) -1),  
  time = "time", 
  event = "status"
 )

# putting them all together
tasks <- list(task_pbc,
              task_veteran,
              task_nki,
              task_lung,
              task_colon_death,
              task_colon_recur,
              # add a few more pre-made ones
              tsk("actg"),
              tsk('gbcs'),
              tsk('grace'),
              tsk("unemployment"),
              tsk("whas"))

```

Now we can make a benchmark designed to compare our three favorite learners:

```{r eval = FALSE}

# Learners with default parameters
learners <- lrns(c("surv.ranger", "surv.rfsrc", "surv.aorsf"))

# Brier (Graf) score, c-index and training time as measures
measures <- msrs(c("surv.graf", "surv.cindex", "time_train"))

# Benchmark with 5-fold CV
design <- benchmark_grid(
  tasks = tasks,
  learners = learners,
  resamplings = rsmps("cv", folds = 5)
)

benchmark_result <- benchmark(design)

bm_scores <- benchmark_result$score(measures, predict_sets = "test")

```

Let's look at the overall results:

```{r eval = FALSE}

bm_scores %>%
 select(task_id, learner_id, surv.graf, surv.cindex, time_train) %>%
 group_by(learner_id) %>% 
 filter(!is.infinite(surv.graf)) %>% 
 summarize(
  across(
   .cols = c(surv.graf, surv.cindex, time_train),
   .fns = ~mean(.x, na.rm = TRUE)
  )
 )


```


```{r echo = FALSE}

tbl_data <-
 structure(
  list(
   learner_id = c("surv.aorsf", "surv.ranger", "surv.rfsrc"),
   surv.graf = c(0.150484184019999, 0.164170239607235, 0.154920952989591),
   surv.cindex = c(0.734037935733795, 0.716040564907922, 0.724149275709082),
   time_train = c(0.382962962962959, 2.04055555555556, 0.757407407407405)
  ),
  row.names = c(NA, -3L),
  class = c("tbl_df", "tbl", "data.frame")
 )

tbl_data

```


From inspection,

- `aorsf` has a higher expected value for 'surv.cindex' (higher is better)

- `aorsf` has a lower expected value for 'surv.graf' (lower is better)


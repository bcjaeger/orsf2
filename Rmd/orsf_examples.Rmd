
# Examples

```{r, echo=FALSE}
knitr::opts_chunk$set(cache = TRUE)
```

First we load some relevant packages

```{r, message=FALSE, warning=FALSE}

set.seed(329730)
library(aorsf)
library(survival)

```


The entry-point into `aorsf` is the standard call to `orsf()`:

```{r}

fit <- orsf(pbc_orsf, Surv(time, status) ~ . - id)

```

printing `fit` provides quick descriptive summaries:

```{r}
fit
```

## More than one way to grow a forest

You can use `orsf(no_fit = TRUE)` to make a specification to grow a forest that can be used as a blueprint for `orsf_train()`:

```{r}

orsf_spec <- orsf(pbc_orsf, time + status ~ . - id, no_fit = TRUE)
orsf_spec

```


```{r}

orsf_fit <- orsf_train(orsf_spec)
orsf_fit

```

You can also update the specification in place when passing it to `orsf_train()`, making it easy to fit several models from one spec:

```{r}

orsf_fit_10 <- orsf_train(orsf_spec, leaf_min_obs = 10)
orsf_fit_20 <- orsf_train(orsf_spec, leaf_min_obs = 20)

orsf_fit$leaf_min_obs
orsf_fit_10$leaf_min_obs
orsf_fit_20$leaf_min_obs

```


## tidymodels

```{r, warning=FALSE, echo=TRUE, message=FALSE}

library(tidymodels)
library(tidyverse)
library(randomForestSRC)
library(ranger)
library(riskRegression)

```


This example uses `tidymodels` functions but stops short of using an official `tidymodels` workflow. I am working on getting `aorsf` pulled into the `censored` package and I will update this with real workflows if that happens!

### Comparing ORSF with other learners

Start with a recipe to pre-process data

```{r}

imputer <- recipe(pbc_orsf, formula = time + status ~ .) %>% 
 step_rm(id) %>% 
 step_impute_mean(all_numeric_predictors()) %>%
 step_impute_mode(all_nominal_predictors()) 

```

Next create a 10-fold cross validation object and pre-process the data:

```{r}

# 10-fold cross validation; make a container for the pre-processed data
analyses <- vfold_cv(data = pbc_orsf, v = 10) %>%
 mutate(recipe = map(splits, ~prep(imputer, training = training(.x))),
        train = map(recipe, juice),
        test = map2(splits, recipe, ~bake(.y, new_data = testing(.x))))

analyses

```

Define functions for a 'workflow' with `randomForestSRC`, `ranger`, and `aorsf`. 

```{r}

rfsrc_wf <- function(train, test, pred_horizon){
 
 # rfsrc does not like tibbles, so cast input data into data.frames
 train <- as.data.frame(train)
 test <- as.data.frame(test)
 
 rfsrc(formula = Surv(time, status) ~ ., data = train) %>% 
  predictRisk(newdata = test, times = pred_horizon) %>% 
  as.numeric()
 
}

ranger_wf <- function(train, test, pred_horizon){
 
 ranger(Surv(time, status) ~ ., data = train) %>% 
  predictRisk(newdata = test, times = pred_horizon) %>% 
  as.numeric()
 
}

aorsf_wf <- function(train, test, pred_horizon){
 
 train %>% 
  orsf(Surv(time, status) ~ .,) %>% 
  predict(new_data = test, 
          pred_type = 'risk',
          pred_horizon = pred_horizon) %>% 
  as.numeric()
 
}

```

Run the 'workflows' on each fold:

```{r}

# 5 year risk prediction
ph <- 365.25 * 5

results <- analyses %>% 
 transmute(test, 
           pred_aorsf = map2(train, test, aorsf_wf, pred_horizon = ph),
           pred_rfsrc = map2(train, test, rfsrc_wf, pred_horizon = ph),
           pred_ranger = map2(train, test, ranger_wf, pred_horizon = ph))

```

Next unnest each column to get back a `tibble` with all of the testing data and predictions.

```{r}

results <- results %>% 
 unnest(everything())

glimpse(results)

```

And finish by aggregating the predictions and computing performance in the testing data. Note that I am computing one statistic for all predictions instead of computing one statistic for each fold. This approach is fine when you have smaller testing sets and/or small event counts.

```{r}

Score(
 object = list(aorsf = results$pred_aorsf,
               rfsrc = results$pred_rfsrc,
               ranger = results$pred_ranger),
 formula = Surv(time, status) ~ 1, 
 data = results, 
 summary = 'IPA',
 times = ph
)

```




